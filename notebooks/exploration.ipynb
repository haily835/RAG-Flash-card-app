{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/RAG-Flash-card-app\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-03-17T00:07:59+00:00', 'author': 'Tong Li, Zhaoyang Liu, Yanyan Shen, Xue Wang, Haokun Chen, Sen Huang', 'keywords': 'Application Domains (APP): APP: Other Applications, Data Mining & Knowledge Management (DMKM): DMKM: Mining of Spatial & Temporal or Spatio-Temporal Data', 'moddate': '2024-03-17T00:08:03+00:00', 'subject': 'The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)', 'title': 'MASTER: Market-Guided Stock Transformer for Stock Price Forecasting', 'source': './data/Paper.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1'}, page_content='MASTER: Market-Guided Stock Transformer for Stock Price Forecasting\\nTong Li 1*, Zhaoyang Liu 2, Yanyan Shen 1â€  , Xue Wang 2, Haokun Chen 2, Sen Huang 2\\n1 Shanghai Jiao Tong University\\n2 Alibaba Group\\n{2017lt, shenyy}@sjtu.edu.cn, {jingmu.lzy, xue.w, hankel.chk, huangsen.huang}@alibaba-inc.com\\nAbstract\\nStock price forecasting has remained an extremely challeng-\\ning problem for many decades due to the high volatility of the\\nstock market. Recent efforts have been devoted to modeling\\ncomplex stock correlations toward joint stock price forecast-\\ning. Existing works share a common neural architecture that\\nlearns temporal patterns from individual stock series and then\\nmixes up temporal representations to establish stock correla-\\ntions. However, they only consider time-aligned stock cor-\\nrelations stemming from all the input stock features, which\\nsuffer from two limitations. First, stock correlations often oc-\\ncur momentarily and in a cross-time manner. Second, the fea-\\nture effectiveness is dynamic with market variation, which af-\\nfects both the stock sequential patterns and their correlations.\\nTo address the limitations, this paper introduces MASTER, a\\nMArkert-Guided Stock TransformER, which models the mo-\\nmentary and cross-time stock correlation and leverages mar-\\nket information for automatic feature selection. MASTER el-\\negantly tackles the complex stock correlation by alternatively\\nengaging in intra-stock and inter-stock information aggrega-\\ntion. Experiments show the superiority of MASTER com-\\npared with previous works and visualize the captured realistic\\nstock correlation to provide valuable insights.\\nIntroduction\\nStock price forecasting, which utilizes historical data col-\\nlected from the stock market to predict future trends, is a\\nvital technique for profitable stock investment. Unlike sta-\\ntionary time series that often exhibit regular patterns such\\nas periodicity and steady trends, the dynamics in the stock\\nprice series are intricate because stock prices fluctuate sub-\\nject to multiple factors, including macroeconomic factors,\\ncapital flows, investor sentiments, and events. The mixing\\nof factors interweaves the stock market as a correlated net-\\nwork, making it difficult to precisely predict the individual\\nbehavior of stocks without taking other stocks into account.\\nMost previous works (Feng et al. 2019; Xu et al. 2021;\\nWang et al. 2021, 2022; Wang, Qu, and Chen 2022) in the\\nfield of stock correlation have relied on predefined concepts,\\nrelationships, or rules and established astatic correlation\\ngraph, e.g., stocks in the same industry are connected to each\\n*This work was done during her internship at Alibaba Group.\\nâ€ Corresponding author.\\nCopyright Â© 2024, Association for the Advancement of Artificial\\nIntelligence (www.aaai.org). All rights reserved.\\ntime\\nstock sequences representations predictions\\nattention map\\nor \\ngraph\\ncorrelation module \\nstocks\\nFigure 1: The framework of existing works. The dashed lines\\nrepresent the underlying momentary and cross-time stock\\ncorrelations, which reside between some(stock1, time1),\\n(stock2, time2) pairs.\\nother. While these methods provide insights into the rela-\\ntions between stocks, they do not account for real-time stock\\ncorrelations. For example, different stocks within the same\\nindustry can experience opposite price movements on a par-\\nticular day. Additionally, the pre-defined relationships may\\nnot be generalizable to new stocks in an evolving market\\nwhere events such as company listing, delisting, or changes\\nin the main business happen normally. Another line of re-\\nsearch (Yoo et al. 2021) follows the Transformer architec-\\nture (Vaswani et al. 2017) and uses the self-attention mech-\\nanism to compute dynamic stock correlations. This data-\\ndriven manner is more flexible and applicable to the time-\\nvarying stock sets in the market. Despite different schemes\\nfor establishing stock correlations, the existing methods gen-\\nerally follow a common two-step computation flow. As de-\\npicted in Figure 1, the first step is using a sequential encoder\\nto summarize the historical sequence of stock features and\\nobtain stock representation, and the second step is to refine\\neach stock representation by aggregating information from\\ncorrelated stocks using graph encoders or attention mecha-\\nnism. However, such a flow suffers from two limitations.\\nFirst, existing works distill an overall stock representa-\\ntion and blur the time-specific details of stock sequence,\\nleading to weakness in modeling the de-facto stock corre-\\nlations, which often occursmomentarily and in a cross-time\\nmanner (Bennett, Cucuringu, and Reinert 2022). To be spe-\\ncific, the stock correlation is highly dynamic and may re-\\nside in misaligned time steps rather than holding through the\\nwhole lookback period. This is because the dominating fac-\\ntors of stock prices constantly change, and different stocks\\nmay react to the same factors with different delays. For in-\\nstance, upstream companiesâ€™ stock prices may react faster to\\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\\n162'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-03-17T00:07:59+00:00', 'author': 'Tong Li, Zhaoyang Liu, Yanyan Shen, Xue Wang, Haokun Chen, Sen Huang', 'keywords': 'Application Domains (APP): APP: Other Applications, Data Mining & Knowledge Management (DMKM): DMKM: Mining of Spatial & Temporal or Spatio-Temporal Data', 'moddate': '2024-03-17T00:08:03+00:00', 'subject': 'The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)', 'title': 'MASTER: Market-Guided Stock Transformer for Stock Price Forecasting', 'source': './data/Paper.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2'}, page_content='a shortage of raw materials than those of downstream com-\\npanies, and individual stocks exhibit a lot of catch-up and\\nfall-behind behaviors.\\nSince the stock correlation may underlie between every\\nstock pair and time pair, a straightforward way to simu-\\nlate the momentary and cross-time correlation is to gather\\ntheÏ„ Ã— |S|feature vectors for pair-wise attention computa-\\ntion, where Ï„ is the lookback window length and S is the\\nstock set. However, in addition to the increased computa-\\ntional complexity, this approach faces practical difficulties\\nbecause the stock forecasting task is in intense data hunger.\\nIntuitively, there are only around 250 trading days per year,\\nproducing limited observations on stocks. When the model\\nadopts such a large attention field with insufficient training\\nsamples, it often struggles to optimize and may even fall\\ninto suboptimal solutions. Although clustering approaches\\nlike local sensitive hashing (Kitaev, Kaiser, and Levskaya\\n2020) have been proposed to reduce the size of the attention\\nfield, they are sensitive to initialization, which is a fatal issue\\nin a data-hungry domain like stock forecasting. To address\\nthese challenges, we propose a novel stock transformer ar-\\nchitecture specifically designed for stock price forecasting.\\nRather than directly modeling theÏ„ Ã— |S|attention field or\\nusing clustering-based approximation methods, our model\\naggregates information from different time steps and differ-\\nent stocks alternately to model the realistic stock correlation\\nand facilitate model learning.\\nAnother limitation of existing works is that they ignore\\nthe impact of varying market status. In long-term practice\\nwith the market variation, one essential observation by in-\\nvestors is that the features come into effect and expire dy-\\nnamically. The effectiveness of features influences both the\\nintra-stock sequential pattern and the stock correlation. For\\ninstance, in a bull market, the correlations among stocks are\\nmore significant due to the investorsâ€™ optimism. Traditional\\ninvestors repeatedly conduct statistical examinations to se-\\nlect effective features, which are exhaustive and face a gap\\nwhen integrated with learning-based methods. To save hu-\\nman efforts, we are motivated to equip our stock transformer\\nwith a novel gating mechanism, which incorporates the mar-\\nket information to perform automatic feature selection. We\\nname the proposed method MASTER, standing forMArket-\\nGuided Stock TransformER. To summarize, our main con-\\ntributions are as follows.\\nâ€¢ We propose a novel stock transformer for stock price\\nforecasting to effectively capture the stock correlation. To\\nthe best of our knowledge, we are the first to mine the\\nmomentary and cross-time stock correlation with learning-\\nbased methods.\\nâ€¢ We introduce a novel gating mechanism that integrates\\nmarket information to automatically select relevant features\\nand adapt to varying market scenarios.\\nâ€¢ We conducted experiments to validate the designs of our\\nproposed method and demonstrated its superiority compared\\nto baselines. The visualization results provided valuable in-\\nsights into the real-time dynamics of stock correlations.\\nMethodology\\nProblem Formulation\\nThe indicators of each stock u âˆˆ Sare collected at every\\ntime step t âˆˆ [1, Ï„] to form the feature vector xu,t âˆˆ RF .\\nFollowing existing works on stock market analysis (Feng\\net al. 2018; Sawhney et al. 2020; Huynh et al. 2023), we\\nfocus on the prediction of the change in stock price rather\\nthan the absolute value. The return ratio,Ëœru = (cu,Ï„+d âˆ’\\ncu,Ï„+1)/cu,Ï„+1, is the relative close price change in d days,\\nwhere cu,t denotes the close price of stock u at time step\\nt, and d represents the predetermined prediction interval.\\nThe return ratio normalizes the market price variety be-\\ntween different stocks in comparison to the absolute price\\nchange. Since stock investment is to rank and select the\\nmost profitable stocks, we perform daily Z-score normaliza-\\ntion of return ratio to encode the label with the rankings,\\nru = NormS(Ëœru), as in previous work (Yang et al. 2020).\\nDefinition 1 (Stock Price Forecasting) Given stock fea-\\ntures {xu,t}uâˆˆS,tâˆˆ[1,Ï„], the stock price forecasting is to\\njointly predict the future normalized return ratio{ru}uâˆˆS.\\nOverview\\nFigure 2 depicts the architecture of our proposed method\\nMASTER, which consists of five steps. (1)Market-Guided\\nGating. We construct a vector representing the current mar-\\nket status mÏ„ and leverage it to rescale feature vectors by\\na gating mechanism, achieving market-guided feature selec-\\ntion. (2)Intra-Stock Aggregation. Within the sequence of\\neach stock, at each time step, we aggregate information from\\nother time steps to generate a local embedding that preserves\\nthe temporal local details of the stock while collecting all\\nimportant signals along the time axis. The local embedding\\nhu,t will serve as relays and transport the collected signals to\\nother stocks in subsequent modules. (3)Inter-Stock Aggre-\\ngation. At each time step, we compute stock correlation with\\nthe attention mechanism, and each stock further aggregates\\nthe local embeddings of other stocks. The aggregated infor-\\nmation zu,t, which we refer to as temporal embedding, con-\\ntains not only the information of the momentarily correlated\\nstocks at t but also preserves the personal information of u.\\n(4) Temporal Aggregation. For each stock, the last tempo-\\nral embedding queries from all historical temporal embed-\\ndings to produce a comprehensive stock embeddingeu. (5)\\nPrediction. The comprehensive stock embedding is sent to\\nprediction layers for label prediction. We elaborate on the\\ndetails of MASTER step by step in the following subsec-\\ntions.\\nMarket-Guided Gating\\nMarket Status Representation First, we propose to com-\\nbine information from two aspects into a vector mÏ„ to give\\nan abundant description of the current market status. (1)\\nMarket index price. The market index price is a weighted\\naverage of the prices of a group of stocksSâ€² by their share\\nof market capitalization. Sâ€² is typically composed of top\\ncompanies with the most market capitalization, represent-\\ning a particular market or sector, and may differ from user-\\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\\n163'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-03-17T00:07:59+00:00', 'author': 'Tong Li, Zhaoyang Liu, Yanyan Shen, Xue Wang, Haokun Chen, Sen Huang', 'keywords': 'Application Domains (APP): APP: Other Applications, Data Mining & Knowledge Management (DMKM): DMKM: Mining of Spatial & Temporal or Spatio-Temporal Data', 'moddate': '2024-03-17T00:08:03+00:00', 'subject': 'The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)', 'title': 'MASTER: Market-Guided Stock Transformer for Stock Price Forecasting', 'source': './data/Paper.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3'}, page_content='ğ‘¦1,1 ğ‘¦1,ğœ\\ntime\\nğ‘¦1,2 â€¦\\nâ„1,1\\nstocks\\nğ‘§1,1 ğ‘§1,ğœğ‘§1,2\\nğ‘’ğ‘¢\\nğ‘’1\\n4. Temporal Aggregation\\n2. Intra-Stock Aggregation\\n5. Prediction\\nâ„ğ‘¢,1\\nâ„1,2\\nâ„ğ‘¢,2\\nâ„1,ğœ\\nâ„ğ‘¢,ğœ\\nğ‘¢\\nğ‘£\\nğ‘–\\nğ‘—\\nğ‘§ğ‘¢,ğ‘–\\nâ„ğ‘£,ğ‘–\\nyğ‘£,ğ‘—\\nTime Step ğ’Š\\nStock ğ’—\\ncross-time correlationÆ¸ğ‘Ÿğ‘¢\\nÆ¸ğ‘Ÿ1\\n3. Inter-Stock Aggregation\\n1. Market-Guided Gating\\nğ‘šğœ Gate\\n\\u0de4ğ‘¥1,1\\nğ‘¥1,1\\nFeature Layer\\nâ€¦\\nâ€¦\\nâ€¦\\nFigure 2: Overview of the MASTER framework.\\ninterested stocks in investing S. We include both the cur-\\nrent market index price at Ï„ and the historical market index\\nprices, which are described by the average and standard de-\\nviation in the past dâ€² days to reveal the fluctuations. Here, dâ€²\\nspecifies the referable interval length to introduce historical\\nmarket information in applications. (2) Market index trading\\nvolume. The trading volumes ofSâ€² reveal the investors in-\\nvolvement, reflecting the activity of the market. We include\\nthe average and standard deviation of market index trading\\nvolume in the pastdâ€² days, to reveal the actual size of the\\nmarket. Sâ€² and dâ€² are identical to the aforementioned defi-\\nnitions. Now we present the market-guided stock price fore-\\ncasting task.\\nDefinition 2 (Market-Guided Stock Price Forecasting)\\nGiven {xu,t}uâˆˆS,tâˆˆ[1,Ï„] and the constructed market status\\nvector mÏ„ , market-guided stock price forecasting is to\\njointly predict the future normalized return ratio{ru}uâˆˆS.\\nGating Mechanism The gating mechanism generates one\\nscaling coefficient for each feature dimension to enlarge or\\nshrink the magnitude of the feature, thereby emphasizing\\nor diminishing the amount of information from the feature\\nflowing to the subsequent modules. The gating mechanism\\nis learned by the model training, and the coefficient is op-\\ntimized by how much the feature contributes to improve\\nforecasting performance, thus reflecting the feature effec-\\ntiveness.\\nGiven the market status representation mÏ„ , |mÏ„ | = Fâ€²,\\nwe first use a single linear layer to transform mÏ„ into the\\nfeature dimension F = |xu,t|. Then, we perform Softmax\\nalong the feature dimension to obtain a distribution.\\nÎ±(mÏ„ ) =F Â· softmaxÎ²(WÎ±mÏ„ + bÎ±),\\nwhere WÎ±, bÎ± are learnable matrix and bias,Î² is the temper-\\nature hyperparameter controlling the sharpness of the output\\ndistribution. Softmax compels competition among features\\nto distinguish the effective ones and ineffective ones. Here,\\na smaller temperatureÎ² encourages the distribution to fo-\\ncus on certain dimensions and the gating effect is stronger\\nwhile a largerÎ² makes the distribution inclined to even and\\nthe gating effect is weaker. Note that we enlarge the value at\\neach dimension byF times as the scaling coefficient. This\\noperation compares the generated distribution with a uni-\\nform distribution where each dimension is1/F, to deter-\\nmine whether to enlarge or shrink the value. The intuition\\nto generate coefficients frommÏ„ is that the effectiveness of\\nfeatures are influenced by market status. For example, if the\\nmodel learns moving average (MA) factor is useful during\\nvolatile market periods, it will emphasize MA when the mar-\\nket becomes volatile again. Under the samemÏ„ , Î± are shared\\nfor {xu,t}, u âˆˆ S, t âˆˆ [1, Ï„], in that we incorporate the most\\nrecent market status to perform unified feature selection. The\\nrescaled feature vectors areËœxu,t = Î±(mÏ„ ) â—¦ xu,t, where â—¦ is\\nthe Hadamard product.\\nIntra-Stock Aggregation\\nIn MASTER, we use intra-stock aggregation followed by\\ninter-stock aggregation to break down the large and complex\\nattention field. Although the entire market is complicated\\nwith diverse behaviors of individual stocks, the patterns of\\na specific stock tend to be relatively continuous. Therefore,\\nwe perform intra-stock aggregation first due to its smaller at-\\ntention field and simpler distribution. In our proposed intra-\\nstock aggregation, the feature at each time step aggregates\\ninformation from other time steps and forms a local embed-\\nding. Compared with existing works which initially mix the\\nfeature sequence into one representation (Yoo et al. 2021),\\nwe maintain a sequence of local embeddings which are ad-\\nvised with the important signals in sequence through intra-\\nstock aggregation while reserving the local details.\\nWe first send the rescaled feature vectors to a feature en-\\ncoder and transform them into the embedding space, yu,t =\\nf(Ëœxu,t), |yu,t| = D. We simply use a single linear layer as\\nf(Â·). Then, we apply a bi-directional sequential encoder to\\nobtain the local output at each time step t. Inspired by the\\nsuccess of transformer-based models in modeling sequential\\npatterns, we instantiate the sequential encoder with a single-\\nlayer transformer encoder (Vaswani et al. 2017). Each fea-\\nture vector at a particular time step is treated as a token, and\\nwe add a fixedD-dimensional sinusoidal positional encod-\\ning pt to mark the chronic order in the lookback window.\\nYu = ||tâˆˆ[1,Ï„]LN(f(Ëœxu,t) +pt),\\nwhere || denotes the concatenation of vectors and LN the\\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\\n164'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-03-17T00:07:59+00:00', 'author': 'Tong Li, Zhaoyang Liu, Yanyan Shen, Xue Wang, Haokun Chen, Sen Huang', 'keywords': 'Application Domains (APP): APP: Other Applications, Data Mining & Knowledge Management (DMKM): DMKM: Mining of Spatial & Temporal or Spatio-Temporal Data', 'moddate': '2024-03-17T00:08:03+00:00', 'subject': 'The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)', 'title': 'MASTER: Market-Guided Stock Transformer for Stock Price Forecasting', 'source': './data/Paper.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4'}, page_content='layer normalization. Then, the feature embedding at each\\ntime step queries from all time steps in the stock sequence.\\nWe introduce multi-head attention mechanisms, denoted as\\nMHA(Â·), with N1 heads to perform different aggregations in\\nparallel. We also utilize feed-forward layers,FFN(Â·), to fuse\\nthe information obtained from the multi-head attention.\\nQ1\\nu = W1\\nQYu, K 1\\nu = W1\\nKYu, V 1\\nu = W1\\nV Yu,\\nH1\\nu = ||tâˆˆ[1,Ï„]hu,t = FFN1(MHA1(Q1\\nu, K1\\nu, V1\\nu ) +Yu),\\nwhere FFN is a two-layer MLP with ReLU activation and\\nresidual connection. As a result, the local embedding hu,t\\nboth reserves the local details and encodes indicative signals\\nfrom other time steps.\\nInter-Stock Aggregation\\nThen, we consider aggregating information from correlated\\nstocks. Compared with existing works that distill an over-\\nall stock correlation, we establish a series of momentary\\nstock correlations corresponding to every time step. Instead\\nof using pre-defined relationships that face a mismatch with\\nthe proximity of real-time stock movements, we propose to\\nmine the asymmetric and dynamic inter-stock correlation\\nvia attention mechanism. The quality of the correlation will\\nbe measured by its contribution to improving the forecast-\\ning performance, and automatically optimized by the model\\ntraining process.\\nSpecifically, at each time step, we gather the local embed-\\ndings of all stocks H2\\nt = ||uâˆˆShu,t and perform multi-head\\nattention mechanism with N2 heads.\\nQ2\\nt = W2\\nQH2\\nt , K 2\\nt = W2\\nKH2\\nt , V 2\\nt = W2\\nV H2\\nt ,\\nZt = ||uâˆˆSzu,t = FFN2(MHA2(Q2\\nt, K2\\nt , V2\\nt ) +H2\\nt ).\\nWith the residual connection of FFN, the temporal embed-\\nding zu,t is encoded with both the information from momen-\\ntarily correlated stocks and the personal information of stock\\nu itself. Our stock transformer is able to model the cross-\\ntime correlation of stocks, as shown in Figure 2 (Right). The\\nlocal details ofyv,j can first be conveyed tohv,i by the intra-\\nstock aggregation of stock v, and then transmitted to zu,i\\nby inter-stock aggregation at time step i, hence modeling\\nthe correlation from any (v, j) to (u, i). We further visualize\\nand explain the captured cross-time correlation in the exper-\\niments section.\\nTemporal Aggregation\\nIn contrast with existing works which obtain one embedding\\nfor each stock after modeling stock correlation (Feng et al.\\n2019), our approach produces a series of temporal embed-\\ndingszu,t, tâˆˆ [1, Ï„]. Each zu,t is encoded with information\\nfrom stocks that are momentarily correlated with (u, t). To\\nsummarize the obtained temporal embeddings, we employ a\\ntemporal attention layer along the time axis.\\nÎ»u,t = exp(zT\\nu,tWÎ»zu,Ï„ )\\nP\\niâˆˆ[1,Ï„] exp(zT\\nu,iWÎ»zu,Ï„ ), e u =\\nX\\ntâˆˆ[1,Ï„]\\nÎ»u,tzu,t,\\nwhere we use the latest temporal embedding zu,Ï„ as the\\nquery vector, and compute the attention score Î»u,t in a hid-\\nden space with transformation matrix WÎ».\\nPrediction and Training\\nFinally, the comprehensive stock embedding eu is fed into\\na predictor g(Â·) for label regression. We use a single linear\\nlayer as the predictor, and the forecasting quality is mea-\\nsured by the MSE loss. In each batch, MASTER is jointly\\noptimized for all u âˆˆ Son a particular prediction date. A\\ntraining epoch is composed of multiple batches correspond-\\ning to different prediction dates in the training set.\\nË†ru = g(eu), L =\\nX\\nuâˆˆS\\nMSE(ru, Ë†ru).\\nDiscussions\\nRelationships with Existing Works Modeling stock cor-\\nrelations has long been an indispensable research direc-\\ntion for stock price prediction. Today, many researchers\\nand quantitative analysts, still opt for linear models, sup-\\nport vector machines, and tree-based methods for stock price\\nforecasting (Nugroho, Adji, and Fauziati 2014; Chen and\\nGuestrin 2016; Kamble 2017; Xie et al. 2013; Li et al. 2015;\\nPiccolo 1990). The aggregation of correlation information\\nwithin and between stocks is often achieved through fea-\\nture engineering, which relies heavily on manual expertise\\nand constantly faces the risk of factor decay. Inspired by\\nthe success of neural sequential data analysis, researchers\\nare driven to take into account the stock feature sequences\\nand learn the temporal correlation automatically. They de-\\nsign various sequential models, such as RNN-based (Feng\\net al. 2019; Sawhney et al. 2021; Yoo et al. 2021; Huynh\\net al. 2023), CNN-based (Wang et al. 2021), and attention-\\nbased models(Liu et al. 2019; Ding et al. 2020), to mine the\\ninternal temporal dynamics of a stock. Recent researches\\nfocus on the modeling of stock correlation, which adds\\na correlation module posterior to the sequential model as\\nillustrated in Figure 1. They propose to use graph-based\\n(Feng et al. 2019; Xu et al. 2021; Wang et al. 2021, 2022),\\nhypergraph-based (Sawhney et al. 2021; Huynh et al. 2023)\\nand attention-based (Yoo et al. 2021; Xiang et al. 2022)\\nmodules to build the overall stock correlation and perform\\njoint prediction. Our MASTER is dedicated to momentary\\nand cross-time stock correlation mining. To do so, we de-\\nvelop a novel model architecture as in Figure 2 that is\\ngenuinely different from all existing methods. Furthermore,\\nMASTER is specialized for stock price forecasting, which\\nis distinct in data form and task properties from existing\\ntransformer-based models in spatial-temporal data (Bulat\\net al. 2021; Cong et al. 2021; Xu et al. 2020; Li et al. 2023)\\nor multivariate time series domains (Zhang and Yan 2022;\\nNie et al. 2022).\\nComplexity Analysis We now analyze the computation\\ncomplexity of our proposed method. Let M = |S|, the\\nmarket-guided gating rescale M Ã— Ï„ feature vectors of\\ndimension F. In intra-stock aggregation, the calculation\\namount of pair-wise attention is Ï„2 for each stock at\\neach attention head. In inter-stock aggregation, the calcu-\\nlation amount is M2 at each time step and each atten-\\ntion head. In temporal aggregation, we compute Ï„ atten-\\ntion scores for each stock. The overall computation com-\\nplexity is O(F MÏ„+ N1MÏ„ 2D2 + N2M2Ï„D2 + MÏ„D 2),\\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\\n165'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-03-17T00:07:59+00:00', 'author': 'Tong Li, Zhaoyang Liu, Yanyan Shen, Xue Wang, Haokun Chen, Sen Huang', 'keywords': 'Application Domains (APP): APP: Other Applications, Data Mining & Knowledge Management (DMKM): DMKM: Mining of Spatial & Temporal or Spatio-Temporal Data', 'moddate': '2024-03-17T00:08:03+00:00', 'subject': 'The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)', 'title': 'MASTER: Market-Guided Stock Transformer for Stock Price Forecasting', 'source': './data/Paper.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5'}, page_content='where M â‰« Ï„. Therefore, MASTER is of O(N2M2Ï„D2)\\ntime complexity. Compared with directly operating on the\\nM Ã— Ï„ attention field with N attention heads, which is\\nin O(NM 2Ï„2D2), we reduce the computation cost by\\nabout Ï„ times and achieve modeling cross-time correla-\\ntions between stocks more efficiently. The overall param-\\neters to be trained in MASTER are transformation matri-\\ncesW1\\nQ, W1\\nK, W1\\nV , W2\\nQ, W2\\nK, W2\\nV , WÎ», which is in shape\\nD Ã— D, and parameters in MLP layers Î±, f,FFN1, FFN2\\nand g.\\nExperiments\\nIn this section, we conduct experiments to answer the fol-\\nlowing four research questions:\\nâ€¢ RQ1 How is the overall performance of MASTER com-\\npared with state-of-the-art methods?\\nâ€¢ RQ2 Is the proposed stock transformer architecture ef-\\nfective for stock price forecasting?\\nâ€¢ RQ3 How do hyper-parameter configurations affect the\\nperformance of MASTER?\\nâ€¢ RQ4 What insights on the stock correlation can we get\\nthrough visualizing the attention map?\\nDatasets We evaluate our framework on the Chinese stock\\nmarket with CSI300 and CSI800 stock sets. CSI300 and\\nCSI800 are two stock sets containing 300 and 800 stocks\\nwith the highest capital value on the Shanghai Stock Ex-\\nchange and the Shenzhen Stock Exchange. The dataset\\ncontains daily information ranging from 2008 to 2022 of\\nCSI300 and CSI800. We use the data from Q1 2008 to Q1\\n2020 as the training set, data in Q2 2020 as the validation\\nset, and the last ten quarters, i.e., Q3 2020 to Q4 2022, are\\nreserved as the test set. We apply the public Alpha158 in-\\ndicators (Yang et al. 2020) to extract stock features from\\nthe collected data. The lookback window lengthÏ„ and pre-\\ndiction interval d are set as 8 and 5 respectively. For mar-\\nket representation, we constructed 63 features with CSI300,\\nCSI500 and CSI800 market indices, and refereable interval\\ndâ€² = 5,10, 20, 30, 60.\\nBaselines We compare the performance of MASTER with\\nseveral stock price forecasting baselines from different cate-\\ngories.â€¢ XGBoost (Chen and Guestrin 2016): A decision-\\ntree based method. According to the leaderboard of Qlib\\nplatform (Yang et al. 2020), it is one of the strongest base-\\nlines. â€¢ LSTM (Graves and Graves 2012), GRU (Cho et al.\\n2014), TCN (Bai, Kolter, and Koltun 2018), and Trans-\\nformer (Vaswani et al. 2017): Sequential baselines that lever-\\nage vanilla LSTM/GRU/temporal convolutional network/-\\nTransformer along the time axis for stock price forecasting.\\nâ€¢GAT (VeliË‡ckoviÂ´c et al. 2017): A graph-based baseline,\\nwhich first uses sequential encoder to gain stock presenta-\\ntion and then aggregates information by graph attention net-\\nworks1. â€¢ DTML (Yoo et al. 2021): A state-of-the-art stock\\ncorrelation mining method, which follows the framework in\\nFigure 1. DTML adopts the attention mechanism to mine the\\n1More discussion is provided in the supplementary materials.\\ndynamic correlation among stocks and also incorporates the\\nmarket information into the modeling.\\nEvaluation We adopt both ranking metrics and portfolio-\\nbased metrics to give a thorough evaluation of the model\\nperformance. Four ranking metrics, Information Coefficient\\n(IC), Rank Information Coefficient (RankIC), Information\\nRatio-based IC (ICIR) and Information Ratio-based RankIC\\n(RankICIR) are considered. IC and RankIC are the Pearson\\ncoefficient and Spearman coefficient averaged at a daily fre-\\nquency. ICIR and RankICIR are normalized metrics of IC\\nand RankIC by dividing the standard deviation. Those met-\\nrics are commonly used in literature (e.g., Xu et al. 2021 and\\nYang et al. 2020) to describe the performance of the forecast-\\ning results from the value and rank perspectives. Further-\\nmore, we employ two portfolio-based metrics to compare\\nthe investment profit and risk of each method. We simulate\\ndaily trading using a simple strategy that selects the top 30\\nstocks with the highest return ratio and reports the Excess\\nAnnualized Return (AR) and Information Ratio (IR) met-\\nrics. AR measures the annual expected excess return gener-\\nated by the investment, while IR measures the risk-adjusted\\nperformance of an investment.\\nImplementation We implemented MASTER 2 with Py-\\nTorch and build our methods based on the open-source quan-\\ntitative investment platform Qlib (Yang et al. 2020). For\\nDTML, we implement it based on the original paper since\\nthere is no official implementation publicly. For other base-\\nlines, we use their Qlib implementations. For hyperparame-\\nters of each baseline method, the layer number and model\\nsize are tuned from {1, 2, 3} and {128, 256, 512} respec-\\ntively. The learning ratelr is tuned among{10âˆ’i}iâˆˆ{3,4,5,6},\\nand we selected the best hyperparameters based on the IC\\nperformance in the validation stage. For hyperparameters of\\nMASTER, we tune the model sizeD and learning rate lr\\namong the same range as the baselines, and the final selec-\\ntion isD=256, lr=10âˆ’5 for all datasets; we set N1=4, N2=2\\nfor all datasets and Î²=5 and Î²=2 for CSI300 and CSI800\\nrespectively. More implementation details of baseline meth-\\nods are summarized in the supplementary materials. Each\\nmodel is trained for at most 40 epochs with early stopping.\\nAll the experiments are conducted on a server equipped with\\nIntel(R) Xeon(R) Platinum 8163 CPU, 128GB Memory, and\\na Tesla V100-SXM2 GPU (16GB Memory). Each experi-\\nment was repeated 5 times with random initialization and\\nthe average performance was reported.\\nOverall Performance (RQ1)\\nThe overall performance is reported in Table 1 MAS-\\nTER achieves the best results on 6/8 of the ranking met-\\nrics, and consistently outperforms all benchmarks in the\\nportfolio-based metrics. In particular, MASTER achieve\\n13%improvements in ranking metrics and 47% improve-\\nments in portfolio-based metrics compared to the second-\\nbest results in the average sense. Note that ranking metrics\\nare computed with the whole set and portfolio-based metrics\\n2Code and supplementary materials are at https://github.com/\\nSJTU-Quant/MASTER\\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\\n166'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-03-17T00:07:59+00:00', 'author': 'Tong Li, Zhaoyang Liu, Yanyan Shen, Xue Wang, Haokun Chen, Sen Huang', 'keywords': 'Application Domains (APP): APP: Other Applications, Data Mining & Knowledge Management (DMKM): DMKM: Mining of Spatial & Temporal or Spatio-Temporal Data', 'moddate': '2024-03-17T00:08:03+00:00', 'subject': 'The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)', 'title': 'MASTER: Market-Guided Stock Transformer for Stock Price Forecasting', 'source': './data/Paper.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6'}, page_content='Dataset Model IC ICIR RankIC RankICIR AR IR\\nCSI300\\nXGBoost 0.051 Â± 0.001 0.37 Â± 0.01 0.050 Â± 0.001 0.36 Â± 0.01 0.23 Â± 0.03 1.9 Â± 0.3\\nLSTM 0.049 Â± 0.001 0.41 Â± 0.01 0.051 Â± 0.002 0.41 Â± 0.03 0.20 Â± 0.04 2 .0 Â± 0.4\\nGRU 0.052 Â± 0.004 0.35 Â± 0.04 0.052 Â± 0.005 0.34 Â± 0.04 0.19 Â± 0.04 1 .5 Â± 0.3\\nTCN 0.050 Â± 0.002 0.33 Â± 0.04 0.049 Â± 0.002 0.31 Â± 0.04 0.18 Â± 0.05 1 .4 Â± 0.5\\nTransformer 0.047 Â± 0.007 0.39 Â± 0.04 0.051 Â± 0.002 0.42 Â± 0.04 0.22 Â± 0.06 2 .0 Â± 0.4\\nGAT 0.054 Â± 0.002 0.36 Â± 0.02 0.041 Â± 0.002 0.25 Â± 0.02 0.19 Â± 0.03 1 .3 Â± 0.3\\nDTML 0.049 Â± 0.006 0.33 Â± 0.04 0.052 Â± 0.005 0.33 Â± 0.04 0.21 Â± 0.03 1 .7 Â± 0.3\\nMASTER 0.064âˆ— Â±0.006 0.42 Â±0.04 0.076 âˆ— Â±0.005 0.49 Â±0.04 0.27 Â±0.05 2.4 Â±0.4\\nCSI800\\nXGBoost 0.040 Â± 0.000 0.37 Â± 0.01 0.047 Â± 0.000 0.42 Â± 0.01 0.08 Â± 0.02 0 .6 Â± 0.2\\nLSTM 0.028 Â± 0.002 0.32 Â± 0.02 0.039 Â± 0.002 0.41 Â± 0.03 0.09 Â± 0.02 0 .9 Â± 0.2\\nGRU 0.039 Â± 0.002 0.36 Â± 0.05 0.044 Â± 0.003 0.39 Â± 0.07 0.07 Â± 0.04 0 .6 Â± 0.3\\nTCN 0.038 Â± 0.002 0.33 Â± 0.04 0.045 Â± 0.002 0.38 Â± 0.05 0.05 Â± 0.04 0 .4 Â± 0.3\\nTransformer 0.040 Â± 0.003 0.43 Â±0.03 0.048 Â± 0.003 0.51 Â±0.05 0.13 Â± 0.04 1 .1 Â± 0.3\\nGAT 0.043 Â± 0.002 0.39 Â± 0.02 0.042 Â± 0.002 0.35 Â± 0.02 0.10 Â± 0.04 0 .7 Â± 0.3\\nDTML 0.039 Â± 0.004 0.29 Â± 0.03 0.053 Â± 0.008 0.37 Â± 0.06 0.16 Â± 0.03 1.3 Â± 0.2\\nMASTER 0.052âˆ— Â±0.006 0.40 Â± 0.06 0.066 Â±0.007 0.48 Â± 0.06 0.28âˆ— Â±0.02 2.3 âˆ— Â±0.3\\nTable 1: Overall performance comparison. The best results are in bold and the second-best results are underlined. And * denotes\\nstatistically significant improvement (measured by t-test with p-value < 0.01) over all baselines.\\nModel IC ICIR RankIC RankICIR AR IR\\n(MA)STER 0.064 Â±0.003 0.43 Â±0.02 0.074 Â±0.004 0.48 Â±0.04 0.25 Â±0.03 2.1 Â±0.3\\n(MA)STER-Bi 0.058 Â± 0.005 0.38 Â± 0.04 0.066 Â± 0.008 0.41 Â± 0.05 0.19 Â± 0.03 1.6 Â± 0.2\\nNaive 0.041 Â± 0.008 0.30 Â± 0.05 0.046 Â± 0.007 0.32 Â± 0.04 0.18 Â± 0.05 1.6 Â± 0.6\\nClustering 0.044 Â± 0.003 0.36 Â± 0.02 0.049 Â± 0.005 0.39 Â± 0.04 0.18 Â± 0.04 1.7 Â± 0.3\\nTable 2: Experiments on CSI300 to validate the effectiveness of proposed stock transformer architecture. The best results are in\\nbold and the second-best results are underlined.\\nmostly consider the 30 top-performed stocks. The achieve-\\nments in both types of metrics imply that MASTER is of\\ngood predicting ability on the whole stock set without sacri-\\nficing the accuracy of the important stocks. The significant\\nimprovements cast light on the importance of stock corre-\\nlation modeling, so that each stock can also benefit from\\nthe historical signals of other momentarily correlated stocks.\\nWe also observed all methods gain better performance on\\nCSI300 over CSI800. We believe it is because CSI300 con-\\nsists of companies with larger capitalization whose stock\\nprices are more predictable. When compared to the exist-\\ning stock correlation method (i.e., DTML), MASTER out-\\nperforms in all 6 metrics, which tells the proposed Market-\\nGuided Gating and aggregation techniques are more effi-\\ncient in mining cross-stock information than existing liter-\\nature.\\nStock Transformer Architecture (RQ2)\\nWe validate the effectiveness of our specialized stock trans-\\nformer architecture by experiments on four settings. (1)\\n(MA)STER, which is our stock transformer without the gat-\\ning. (2) (MA)STER-Bi, in which we substitute the single-\\nlayer transformer encoder with a bi-directional LSTM to\\nevince that the effectiveness of our proposed architecture\\nis not coupled with strong sequential encoders. (3) Naive,\\nwhich directly performs information aggregation amongÏ„ Ã—\\n|S| tokens. (4) Clustering, in which we adapt the Local Sen-\\nsitive Hashing (Kitaev, Kaiser, and Levskaya 2020) to allo-\\ncate all tokens into 10 buckets by similarity and perform ag-\\ngregation within each group, which is a classic task-agnostic\\ntechnique to reduce the scale of the attention field. For a fair\\ncomparison, in (3) and (4), we first use the same transformer\\nencoder to extract token embedding and then use the same\\nmulti-head attention mechanism as in our stock transformer,\\nso the only difference is the attention field. Due to resource\\nlimits, we only experiment on CSI300 dataset. The results\\nin Table ?? illustrate the efficacy of our tailored stock trans-\\nformer architecture, which performs intra-stock aggregation\\nand inter-stock aggregation alternatively.\\nAblation Study (RQ3)\\nFirst, we conduct ablation study on (N1, N2) combination.\\nThe results of CSI300 are shown in Figure 3 and the results\\non CSI800 are similar. The difference among head combina-\\ntions is not significant compared with the inherent variance\\nunder each setting. In the studied range, most settings con-\\nsistently performed better than the baselines.\\nSecond, we study the influence of temperature Î² in the\\ngating mechanism. As explained before, a smaller Î² forces\\na stronger feature selection while a largerÎ² turns off the gat-\\ning effect. Figure 4 shows the performance with varying Î².\\nThe CSI300 is a relatively easier dataset where most fea-\\ntures are quite effective, so the temperature is expected to\\nbe larger to relax the feature selection, while more powerful\\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\\n167'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-03-17T00:07:59+00:00', 'author': 'Tong Li, Zhaoyang Liu, Yanyan Shen, Xue Wang, Haokun Chen, Sen Huang', 'keywords': 'Application Domains (APP): APP: Other Applications, Data Mining & Knowledge Management (DMKM): DMKM: Mining of Spatial & Temporal or Spatio-Temporal Data', 'moddate': '2024-03-17T00:08:03+00:00', 'subject': 'The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)', 'title': 'MASTER: Market-Guided Stock Transformer for Stock Price Forecasting', 'source': './data/Paper.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7'}, page_content='N11\\n2\\n4\\nN2\\n1\\n2\\n4\\n0.056\\n0.064\\nIC\\nN11\\n2\\n4\\nN2\\n1\\n2\\n4\\n0.40\\n0.44\\nICIR\\nN11\\n2\\n4\\nN2\\n1\\n2\\n4\\n0.06\\n0.07\\n0.08\\nRankIC\\nN11\\n2\\n4\\nN2\\n1\\n2\\n4\\n0.40\\n0.48\\nRankICIR\\nN11\\n2\\n4\\nN2\\n1\\n2\\n4\\n0.18\\n0.24\\n0.30\\nAR\\nN11\\n2\\n4\\nN2\\n1\\n2\\n4\\n1.6\\n2.0\\n2.4\\nIR\\nFigure 3: The average and standard deviation of metrics with different (N1, N2) combinations on CSI300.\\n1 2 5 10 20\\n0.040\\n0.048\\n0.056\\n0.064\\nIC\\n1 2 5 10 20\\n0.32\\n0.36\\n0.40\\n0.44\\nICIR\\n1 2 5 10 20\\n0.056\\n0.064\\n0.072\\n0.080\\nRankIC\\n1 2 5 10 20\\n0.40\\n0.44\\n0.48\\n0.52\\nRankICIR\\n1 2 5 10 20\\n0.15\\n0.20\\n0.25\\n0.30\\nAR\\n1 2 5 10 20\\n1.2\\n1.6\\n2.0\\n2.4\\n2.8\\nIR\\nCSI300\\nCSI800\\nFigure 4: MASTER performance with varying Î². The horizontal dash lines are performance without market-guided gating.\\n1\\n8\\nCNPC(SH601857)\\n1\\n8\\nICBC(SH601398)\\n1\\n8\\nCATL(SZ300750)\\nAvg.\\n0.01\\n0.02\\n0.03\\nFigure 5: The correlation towards three target stocks on Aug\\n19th, 2022. The y-axis is time steps in the lookback win-\\ndow and the x-axis is source stocks.Avg. denotes the evenly\\ndistributed value.\\nfeature selection intervention is needed for the sophisticated\\nCSI800 dataset whose Î² of the best performance is smaller.\\nVisualization of Attention Maps (RQ4)\\nWe show how MASTER captures the momentary and cross-\\ntime stock correlation that previous methods are not ex-\\npressive enough to model. Figure 5 shows the inter-stock\\nattention map at different time steps in the lookback win-\\ndow. We choose three representative stocks as the target\\nand sample100 random stocks as sources for visualiza-\\ntion. The highlighted part is scattered instead of exhibit-\\ning neat strips, implying that the correlation is momentary\\nrather than long-standing. Also, the inter-stock correlation\\nis sparse, with only a few stocks having strong correlations\\ntoward the target stocks. Figure 6 displays the correlation\\nbetween stock pairs to show how the correlation resides in\\ntime. From source stock v to target stock u, we compute\\nIuâ†v[i, j] = S1\\nv[i, j]S2\\ni [u, v] as the Ï„ Ã— Ï„ correlation map,\\nwhile S1 and S2 are the intra-stock and inter-stock attention\\nmap. First, the highlighted blocks are not centered on the di-\\nagonal, because the stock correlation is usually cross-time\\nrather than temporally aligned. Second, the left two figures\\n1 2 3 4 5 6 7 8\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\nCNPC CATL, 19th\\n1 2 3 4 5 6 7 8\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\nCATL CNPC, 19th\\n1 2 3 4 5 6 7 8\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\nCNPC ICBC, 19th\\n1 2 3 4 5 6 7 8\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\nCNPC ICBC, 25th\\nAvg.\\n0.001\\n0.002\\nFigure 6: Cross-time correlation of stock pairs on Aug 19th\\nand 25th, 2022. The x-axis is the source time steps and the\\ny-axis is the target time steps.\\nare totally different, illustrating that correlations are highly\\nasymmetric between u â† v and v â† u. Third, the impor-\\ntance of mined correlation changes slowly when the look-\\nback window slides to forecast on different dates. For exam-\\nple, blocked regions in the right two figures correspond to\\nthe same absolute time scope of different prediction dates,\\nwhose patterns are to a certain degree similar.\\nConclusion\\nWe introduce a novel method MASTER for stock price fore-\\ncasting, which models the realistic stock correlation and\\nguides feature selection with market information. MAS-\\nTER consists of five steps, market-guided gating, intra-stock\\naggregation, inter-stock aggregation, temporal aggregation,\\nand prediction. Experiments on the Chinese market with2\\nstock universe show that MASTER achieves averagely13%\\nimprovements on ranking metrics and 47% on portfolio-\\nbased metrics compared with all baselines. Visualization of\\nattention maps reveals the de-facto momentary and cross-\\ntime stock correlation. In conclusion, we provide a more\\ngranular perspective for studying stock correlation, while\\nalso indicating an effective application of market informa-\\ntion. Future work can explore mining stock correlations of\\nhigher quality and study other uses of market information.\\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\\n168'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-03-17T00:07:59+00:00', 'author': 'Tong Li, Zhaoyang Liu, Yanyan Shen, Xue Wang, Haokun Chen, Sen Huang', 'keywords': 'Application Domains (APP): APP: Other Applications, Data Mining & Knowledge Management (DMKM): DMKM: Mining of Spatial & Temporal or Spatio-Temporal Data', 'moddate': '2024-03-17T00:08:03+00:00', 'subject': 'The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)', 'title': 'MASTER: Market-Guided Stock Transformer for Stock Price Forecasting', 'source': './data/Paper.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8'}, page_content='Acknowledgements\\nThe authors would like to thank the anonymous review-\\ners for their insightful reviews. This work is supported\\nby the National Key Research and Development Program\\nof China (2022YFE0200500), Shanghai Municipal Sci-\\nence and Technology Major Project (2021SHZDZX0102),\\nand SJTU Global Strategic Partnership Fund (2021SJTU-\\nHKUST).\\nReferences\\nBai, S.; Kolter, J. Z.; and Koltun, V . 2018. An empirical\\nevaluation of generic convolutional and recurrent networks\\nfor sequence modeling.arXiv preprint arXiv:1803.01271.\\nBennett, S.; Cucuringu, M.; and Reinert, G. 2022. Leadâ€“\\nlag detection and network clustering for multivariate time\\nseries with an application to the US equity market.Machine\\nLearning, 111(12): 4497â€“4538.\\nBulat, A.; Perez Rua, J. M.; Sudhakaran, S.; Martinez, B.;\\nand Tzimiropoulos, G. 2021. Space-time mixing attention\\nfor video transformer. Advances in neural information pro-\\ncessing systems, 34: 19594â€“19607.\\nChen, T.; and Guestrin, C. 2016. Xgboost: A scalable tree\\nboosting system. In Proceedings of the 22nd acm sigkdd\\ninternational conference on knowledge discovery and data\\nmining, 785â€“794.\\nCho, K.; Van Merri Â¨enboer, B.; Gulcehre, C.; Bahdanau,\\nD.; Bougares, F.; Schwenk, H.; and Bengio, Y . 2014.\\nLearning phrase representations using RNN encoder-\\ndecoder for statistical machine translation. arXiv preprint\\narXiv:1406.1078.\\nCong, Y .; Liao, W.; Ackermann, H.; Rosenhahn, B.; and\\nYang, M. Y . 2021. Spatial-temporal transformer for dynamic\\nscene graph generation. InProceedings of the IEEE/CVF in-\\nternational conference on computer vision, 16372â€“16382.\\nDing, Q.; Wu, S.; Sun, H.; Guo, J.; and Guo, J. 2020. Hier-\\narchical Multi-Scale Gaussian Transformer for Stock Move-\\nment Prediction. In IJCAI, 4640â€“4646.\\nFeng, F.; Chen, H.; He, X.; Ding, J.; Sun, M.; and Chua,\\nT.-S. 2018. Enhancing stock movement prediction with ad-\\nversarial training.arXiv preprint arXiv:1810.09936.\\nFeng, F.; He, X.; Wang, X.; Luo, C.; Liu, Y .; and Chua, T.-\\nS. 2019. Temporal relational ranking for stock prediction.\\nACM Transactions on Information Systems (TOIS), 37(2):\\n1â€“30.\\nGraves, A.; and Graves, A. 2012. Long short-term mem-\\nory. Supervised sequence labelling with recurrent neural\\nnetworks, 37â€“45.\\nHuynh, T. T.; Nguyen, M. H.; Nguyen, T. T.; Nguyen, P. L.;\\nWeidlich, M.; Nguyen, Q. V . H.; and Aberer, K. 2023. Ef-\\nficient integration of multi-order dynamics and internal dy-\\nnamics in stock movement prediction. InProceedings of the\\nSixteenth ACM International Conference on Web Search and\\nData Mining, 850â€“858.\\nKamble, R. A. 2017. Short and long term stock trend predic-\\ntion using decision tree. In 2017 International Conference\\non Intelligent Computing and Control Systems (ICICCS) ,\\n1371â€“1375. IEEE.\\nKitaev, N.; Kaiser, Å.; and Levskaya, A. 2020. Reformer:\\nThe efficient transformer. arXiv preprint arXiv:2001.04451.\\nLi, L.; Duan, L.; Wang, J.; He, C.; Chen, Z.; Xie, G.;\\nDeng, S.; and Luo, Z. 2023. Memory-Enhanced Trans-\\nformer for Representation Learning on Temporal Heteroge-\\nneous Graphs.Data Science and Engineering, 8(2): 98â€“111.\\nLi, Q.; Jiang, L.; Li, P.; and Chen, H. 2015. Tensor-based\\nlearning for predicting stock movements. InProceedings of\\nthe AAAI Conference on Artificial Intelligence, volume 29.\\nLiu, J.; Lin, H.; Liu, X.; Xu, B.; Ren, Y .; Diao, Y .; and\\nYang, L. 2019. Transformer-based capsule network for stock\\nmovement prediction. InProceedings of the first workshop\\non financial technology and natural language processing ,\\n66â€“73.\\nNie, Y .; Nguyen, N. H.; Sinthong, P.; and Kalagnanam, J.\\n2022. A Time Series is Worth 64 Words: Long-term Fore-\\ncasting with Transformers. InThe Eleventh International\\nConference on Learning Representations.\\nNugroho, F. S. D.; Adji, T. B.; and Fauziati, S. 2014. Deci-\\nsion support system for stock trading using multiple indica-\\ntors decision tree. In2014 The 1st International Conference\\non Information Technology, Computer, and Electrical Engi-\\nneering, 291â€“296. IEEE.\\nPiccolo, D. 1990. A distance measure for classifying\\nARIMA models. Journal of time series analysis, 11(2): 153â€“\\n164.\\nSawhney, R.; Agarwal, S.; Wadhwa, A.; Derr, T.; and Shah,\\nR. R. 2021. Stock selection via spatiotemporal hypergraph\\nattention network: A learning to rank approach. InProceed-\\nings of the AAAI Conference on Artificial Intelligence, vol-\\nume 35, 497â€“504.\\nSawhney, R.; Agarwal, S.; Wadhwa, A.; and Shah, R. R.\\n2020. Spatiotemporal hypergraph convolution network for\\nstock movement forecasting. In 2020 IEEE International\\nConference on Data Mining (ICDM), 482â€“491. IEEE.\\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\\nL.; Gomez, A. N.; Kaiser, Å.; and Polosukhin, I. 2017. At-\\ntention is all you need.Advances in neural information pro-\\ncessing systems, 30.\\nVeliË‡ckoviÂ´c, P.; Cucurull, G.; Casanova, A.; Romero, A.; Lio,\\nP.; and Bengio, Y . 2017. Graph attention networks. arXiv\\npreprint arXiv:1710.10903.\\nWang, H.; Li, S.; Wang, T.; and Zheng, J. 2021. Hierarchi-\\ncal Adaptive Temporal-Relational Modeling for Stock Trend\\nPrediction. InIJCAI, 3691â€“3698.\\nWang, H.; Wang, T.; Li, S.; Zheng, J.; Guan, S.; and Chen,\\nW. 2022. Adaptive long-short pattern transformer for stock\\ninvestment selection. InProceedings of the Thirty-First\\nInternational Joint Conference on Artificial Intelligence ,\\n3970â€“3977.\\nWang, Y .; Qu, Y .; and Chen, Z. 2022. Review of graph con-\\nstruction and graph learning in stock price prediction. Pro-\\ncedia Computer Science, 214: 771â€“778.\\nXiang, S.; Cheng, D.; Shang, C.; Zhang, Y .; and Liang, Y .\\n2022. Temporal and Heterogeneous Graph Neural Network\\nfor Financial Time Series Prediction. InProceedings of\\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\\n169'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-03-17T00:07:59+00:00', 'author': 'Tong Li, Zhaoyang Liu, Yanyan Shen, Xue Wang, Haokun Chen, Sen Huang', 'keywords': 'Application Domains (APP): APP: Other Applications, Data Mining & Knowledge Management (DMKM): DMKM: Mining of Spatial & Temporal or Spatio-Temporal Data', 'moddate': '2024-03-17T00:08:03+00:00', 'subject': 'The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)', 'title': 'MASTER: Market-Guided Stock Transformer for Stock Price Forecasting', 'source': './data/Paper.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9'}, page_content='the 31st ACM International Conference on Information &\\nKnowledge Management, 3584â€“3593.\\nXie, B.; Passonneau, R.; Wu, L.; and Creamer, G. G. 2013.\\nSemantic frames to predict stock price movement. In Pro-\\nceedings of the 51st annual meeting of the association for\\ncomputational linguistics, 873â€“883.\\nXu, M.; Dai, W.; Liu, C.; Gao, X.; Lin, W.; Qi, G.-J.; and\\nXiong, H. 2020. Spatial-temporal transformer networks for\\ntraffic flow forecasting.arXiv preprint arXiv:2001.02908.\\nXu, W.; Liu, W.; Wang, L.; Xia, Y .; Bian, J.; Yin, J.; and Liu,\\nT.-Y . 2021. Hist: A graph-based framework for stock trend\\nforecasting via mining concept-oriented shared information.\\narXiv preprint arXiv:2110.13716.\\nYang, X.; Liu, W.; Zhou, D.; Bian, J.; and Liu, T.-Y . 2020.\\nQlib: An ai-oriented quantitative investment platform.arXiv\\npreprint arXiv:2009.11189.\\nYoo, J.; Soun, Y .; Park, Y .-c.; and Kang, U. 2021. Accu-\\nrate multivariate stock movement prediction via data-axis\\ntransformer with multi-level contexts. InProceedings of the\\n27th ACM SIGKDD Conference on Knowledge Discovery &\\nData Mining, 2037â€“2045.\\nZhang, Y .; and Yan, J. 2022. Crossformer: Transformer uti-\\nlizing cross-dimension dependency for multivariate time se-\\nries forecasting. In The Eleventh International Conference\\non Learning Representations.\\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\\n170')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.ingest import *\n",
    "raw_texts = load_documents()\n",
    "raw_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = chunk_text(raw_texts)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.embeddings.Embeddings object at 0xffff501f8680> async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0xffff4f933380> model='text-embedding-3-small' dimensions=None deployment='text-embedding-ada-002' openai_api_version=None openai_api_base=None openai_api_type=None openai_proxy=None embedding_ctx_length=8191 openai_api_key=SecretStr('**********') openai_organization=None allowed_special=None disallowed_special=None chunk_size=1000 max_retries=2 request_timeout=None headers=None tiktoken_enabled=True tiktoken_model_name=None show_progress_bar=False model_kwargs={} skip_empty=False default_headers=None default_query=None retry_min_seconds=4 retry_max_seconds=20 http_client=None http_async_client=None check_embedding_ctx_length=True\n"
     ]
    }
   ],
   "source": [
    "embedding_function = get_embedder()\n",
    "print(embedding_function)\n",
    "collection = create_chroma_index(\n",
    "    file_name=\"test\",\n",
    "    db_path='./models/embeddings/chroma_db',\n",
    "    embedding_function=embedding_function, \n",
    "    texts=texts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_function = get_embedder()\n",
    "vectorstore = load_vectorstore(\n",
    "    file_name=\"test\",\n",
    "    vectorstore_path='./models/embeddings/chroma_db',\n",
    "    embedding_function=embedding_function) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='66ff8d31-aad8-5b56-a824-55ba5c8de3e8', metadata={'author': 'Tong Li, Zhaoyang Liu, Yanyan Shen, Xue Wang, Haokun Chen, Sen Huang', 'creationdate': '2024-03-17T00:07:59+00:00', 'creator': 'PyPDF', 'keywords': 'Application Domains (APP): APP: Other Applications, Data Mining & Knowledge Management (DMKM): DMKM: Mining of Spatial & Temporal or Spatio-Temporal Data', 'moddate': '2024-03-17T00:08:03+00:00', 'page': 3, 'page_label': '4', 'producer': 'PyPDF', 'source': './data/Paper.pdf', 'subject': 'The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)', 'title': 'MASTER: Market-Guided Stock Transformer for Stock Price Forecasting', 'total_pages': 9}, page_content='velop a novel model architecture as in Figure 2 that is\\ngenuinely different from all existing methods. Furthermore,\\nMASTER is specialized for stock price forecasting, which\\nis distinct in data form and task properties from existing\\ntransformer-based models in spatial-temporal data (Bulat\\net al. 2021; Cong et al. 2021; Xu et al. 2020; Li et al. 2023)\\nor multivariate time series domains (Zhang and Yan 2022;\\nNie et al. 2022).\\nComplexity Analysis We now analyze the computation\\ncomplexity of our proposed method. Let M = |S|, the\\nmarket-guided gating rescale M Ã— Ï„ feature vectors of\\ndimension F. In intra-stock aggregation, the calculation\\namount of pair-wise attention is Ï„2 for each stock at\\neach attention head. In inter-stock aggregation, the calcu-\\nlation amount is M2 at each time step and each atten-\\ntion head. In temporal aggregation, we compute Ï„ atten-\\ntion scores for each stock. The overall computation com-\\nplexity is O(F MÏ„+ N1MÏ„ 2D2 + N2M2Ï„D2 + MÏ„D 2),\\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\\n165'),\n",
       " Document(id='fae7ce2d-a8e2-5359-a609-c7a97ff0120a', metadata={'author': 'Tong Li, Zhaoyang Liu, Yanyan Shen, Xue Wang, Haokun Chen, Sen Huang', 'creationdate': '2024-03-17T00:07:59+00:00', 'creator': 'PyPDF', 'keywords': 'Application Domains (APP): APP: Other Applications, Data Mining & Knowledge Management (DMKM): DMKM: Mining of Spatial & Temporal or Spatio-Temporal Data', 'moddate': '2024-03-17T00:08:03+00:00', 'page': 0, 'page_label': '1', 'producer': 'PyPDF', 'source': './data/Paper.pdf', 'subject': 'The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)', 'title': 'MASTER: Market-Guided Stock Transformer for Stock Price Forecasting', 'total_pages': 9}, page_content='MASTER: Market-Guided Stock Transformer for Stock Price Forecasting\\nTong Li 1*, Zhaoyang Liu 2, Yanyan Shen 1â€  , Xue Wang 2, Haokun Chen 2, Sen Huang 2\\n1 Shanghai Jiao Tong University\\n2 Alibaba Group\\n{2017lt, shenyy}@sjtu.edu.cn, {jingmu.lzy, xue.w, hankel.chk, huangsen.huang}@alibaba-inc.com\\nAbstract\\nStock price forecasting has remained an extremely challeng-\\ning problem for many decades due to the high volatility of the\\nstock market. Recent efforts have been devoted to modeling\\ncomplex stock correlations toward joint stock price forecast-\\ning. Existing works share a common neural architecture that\\nlearns temporal patterns from individual stock series and then\\nmixes up temporal representations to establish stock correla-\\ntions. However, they only consider time-aligned stock cor-\\nrelations stemming from all the input stock features, which\\nsuffer from two limitations. First, stock correlations often oc-\\ncur momentarily and in a cross-time manner. Second, the fea-\\nture effectiveness is dynamic with market variation, which af-\\nfects both the stock sequential patterns and their correlations.\\nTo address the limitations, this paper introduces MASTER, a\\nMArkert-Guided Stock TransformER, which models the mo-\\nmentary and cross-time stock correlation and leverages mar-\\nket information for automatic feature selection. MASTER el-\\negantly tackles the complex stock correlation by alternatively\\nengaging in intra-stock and inter-stock information aggrega-'),\n",
       " Document(id='503e4798-2bfd-5b94-9e0f-28a5d9c4768e', metadata={'author': 'Tong Li, Zhaoyang Liu, Yanyan Shen, Xue Wang, Haokun Chen, Sen Huang', 'creationdate': '2024-03-17T00:07:59+00:00', 'creator': 'PyPDF', 'keywords': 'Application Domains (APP): APP: Other Applications, Data Mining & Knowledge Management (DMKM): DMKM: Mining of Spatial & Temporal or Spatio-Temporal Data', 'moddate': '2024-03-17T00:08:03+00:00', 'page': 5, 'page_label': '6', 'producer': 'PyPDF', 'source': './data/Paper.pdf', 'subject': 'The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)', 'title': 'MASTER: Market-Guided Stock Transformer for Stock Price Forecasting', 'total_pages': 9}, page_content='We also observed all methods gain better performance on\\nCSI300 over CSI800. We believe it is because CSI300 con-\\nsists of companies with larger capitalization whose stock\\nprices are more predictable. When compared to the exist-\\ning stock correlation method (i.e., DTML), MASTER out-\\nperforms in all 6 metrics, which tells the proposed Market-\\nGuided Gating and aggregation techniques are more effi-\\ncient in mining cross-stock information than existing liter-\\nature.\\nStock Transformer Architecture (RQ2)\\nWe validate the effectiveness of our specialized stock trans-\\nformer architecture by experiments on four settings. (1)\\n(MA)STER, which is our stock transformer without the gat-\\ning. (2) (MA)STER-Bi, in which we substitute the single-\\nlayer transformer encoder with a bi-directional LSTM to\\nevince that the effectiveness of our proposed architecture\\nis not coupled with strong sequential encoders. (3) Naive,\\nwhich directly performs information aggregation amongÏ„ Ã—\\n|S| tokens. (4) Clustering, in which we adapt the Local Sen-\\nsitive Hashing (Kitaev, Kaiser, and Levskaya 2020) to allo-\\ncate all tokens into 10 buckets by similarity and perform ag-\\ngregation within each group, which is a classic task-agnostic\\ntechnique to reduce the scale of the attention field. For a fair\\ncomparison, in (3) and (4), we first use the same transformer\\nencoder to extract token embedding and then use the same\\nmulti-head attention mechanism as in our stock transformer,'),\n",
       " Document(id='86838230-239a-5746-ad49-ab97cbe26161', metadata={'author': 'Tong Li, Zhaoyang Liu, Yanyan Shen, Xue Wang, Haokun Chen, Sen Huang', 'creationdate': '2024-03-17T00:07:59+00:00', 'creator': 'PyPDF', 'keywords': 'Application Domains (APP): APP: Other Applications, Data Mining & Knowledge Management (DMKM): DMKM: Mining of Spatial & Temporal or Spatio-Temporal Data', 'moddate': '2024-03-17T00:08:03+00:00', 'page': 4, 'page_label': '5', 'producer': 'PyPDF', 'source': './data/Paper.pdf', 'subject': 'The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)', 'title': 'MASTER: Market-Guided Stock Transformer for Stock Price Forecasting', 'total_pages': 9}, page_content='which first uses sequential encoder to gain stock presenta-\\ntion and then aggregates information by graph attention net-\\nworks1. â€¢ DTML (Yoo et al. 2021): A state-of-the-art stock\\ncorrelation mining method, which follows the framework in\\nFigure 1. DTML adopts the attention mechanism to mine the\\n1More discussion is provided in the supplementary materials.\\ndynamic correlation among stocks and also incorporates the\\nmarket information into the modeling.\\nEvaluation We adopt both ranking metrics and portfolio-\\nbased metrics to give a thorough evaluation of the model\\nperformance. Four ranking metrics, Information Coefficient\\n(IC), Rank Information Coefficient (RankIC), Information\\nRatio-based IC (ICIR) and Information Ratio-based RankIC\\n(RankICIR) are considered. IC and RankIC are the Pearson\\ncoefficient and Spearman coefficient averaged at a daily fre-\\nquency. ICIR and RankICIR are normalized metrics of IC\\nand RankIC by dividing the standard deviation. Those met-\\nrics are commonly used in literature (e.g., Xu et al. 2021 and\\nYang et al. 2020) to describe the performance of the forecast-\\ning results from the value and rank perspectives. Further-\\nmore, we employ two portfolio-based metrics to compare\\nthe investment profit and risk of each method. We simulate\\ndaily trading using a simple strategy that selects the top 30\\nstocks with the highest return ratio and reports the Excess\\nAnnualized Return (AR) and Information Ratio (IR) met-')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.retriever import retrieve\n",
    "retriever = vectorstore.as_retriever(search_type='similarity')\n",
    "context = retriever.invoke(\"What this author proposed?\")\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35540/3608085082.py:1: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from src.generator import generate_response\n"
     ]
    }
   ],
   "source": [
    "from src.generator import generate_response\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='8703233d-df70-5d09-ac57-9fe8032ce341', metadata={'author': 'Tong Li, Zhaoyang Liu, Yanyan Shen, Xue Wang, Haokun Chen, Sen Huang', 'creationdate': '2024-03-17T00:07:59+00:00', 'creator': 'PyPDF', 'keywords': 'Application Domains (APP): APP: Other Applications, Data Mining & Knowledge Management (DMKM): DMKM: Mining of Spatial & Temporal or Spatio-Temporal Data', 'moddate': '2024-03-17T00:08:03+00:00', 'page': 2, 'page_label': '3', 'producer': 'PyPDF', 'source': './data/Paper.pdf', 'subject': 'The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)', 'title': 'MASTER: Market-Guided Stock Transformer for Stock Price Forecasting', 'total_pages': 9}, page_content='ğ‘¦1,1 ğ‘¦1,ğœ\\ntime\\nğ‘¦1,2 â€¦\\nâ„1,1\\nstocks\\nğ‘§1,1 ğ‘§1,ğœğ‘§1,2\\nğ‘’ğ‘¢\\nğ‘’1\\n4. Temporal Aggregation\\n2. Intra-Stock Aggregation\\n5. Prediction\\nâ„ğ‘¢,1\\nâ„1,2\\nâ„ğ‘¢,2\\nâ„1,ğœ\\nâ„ğ‘¢,ğœ\\nğ‘¢\\nğ‘£\\nğ‘–\\nğ‘—\\nğ‘§ğ‘¢,ğ‘–\\nâ„ğ‘£,ğ‘–\\nyğ‘£,ğ‘—\\nTime Step ğ’Š\\nStock ğ’—\\ncross-time correlationÆ¸ğ‘Ÿğ‘¢\\nÆ¸ğ‘Ÿ1\\n3. Inter-Stock Aggregation\\n1. Market-Guided Gating\\nğ‘šğœ Gate\\n\\u0de4ğ‘¥1,1\\nğ‘¥1,1\\nFeature Layer\\nâ€¦\\nâ€¦\\nâ€¦\\nFigure 2: Overview of the MASTER framework.\\ninterested stocks in investing S. We include both the cur-\\nrent market index price at Ï„ and the historical market index\\nprices, which are described by the average and standard de-\\nviation in the past dâ€² days to reveal the fluctuations. Here, dâ€²\\nspecifies the referable interval length to introduce historical\\nmarket information in applications. (2) Market index trading\\nvolume. The trading volumes ofSâ€² reveal the investors in-\\nvolvement, reflecting the activity of the market. We include\\nthe average and standard deviation of market index trading\\nvolume in the pastdâ€² days, to reveal the actual size of the\\nmarket. Sâ€² and dâ€² are identical to the aforementioned defi-\\nnitions. Now we present the market-guided stock price fore-\\ncasting task.\\nDefinition 2 (Market-Guided Stock Price Forecasting)\\nGiven {xu,t}uâˆˆS,tâˆˆ[1,Ï„] and the constructed market status\\nvector mÏ„ , market-guided stock price forecasting is to\\njointly predict the future normalized return ratio{ru}uâˆˆS.\\nGating Mechanism The gating mechanism generates one\\nscaling coefficient for each feature dimension to enlarge or\\nshrink the magnitude of the feature, thereby emphasizing'),\n",
       " Document(id='0bca9c22-ecb6-5e26-9f69-b4177bc9af19', metadata={'author': 'Tong Li, Zhaoyang Liu, Yanyan Shen, Xue Wang, Haokun Chen, Sen Huang', 'creationdate': '2024-03-17T00:07:59+00:00', 'creator': 'PyPDF', 'keywords': 'Application Domains (APP): APP: Other Applications, Data Mining & Knowledge Management (DMKM): DMKM: Mining of Spatial & Temporal or Spatio-Temporal Data', 'moddate': '2024-03-17T00:08:03+00:00', 'page': 2, 'page_label': '3', 'producer': 'PyPDF', 'source': './data/Paper.pdf', 'subject': 'The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)', 'title': 'MASTER: Market-Guided Stock Transformer for Stock Price Forecasting', 'total_pages': 9}, page_content='information from other time steps and forms a local embed-\\nding. Compared with existing works which initially mix the\\nfeature sequence into one representation (Yoo et al. 2021),\\nwe maintain a sequence of local embeddings which are ad-\\nvised with the important signals in sequence through intra-\\nstock aggregation while reserving the local details.\\nWe first send the rescaled feature vectors to a feature en-\\ncoder and transform them into the embedding space, yu,t =\\nf(Ëœxu,t), |yu,t| = D. We simply use a single linear layer as\\nf(Â·). Then, we apply a bi-directional sequential encoder to\\nobtain the local output at each time step t. Inspired by the\\nsuccess of transformer-based models in modeling sequential\\npatterns, we instantiate the sequential encoder with a single-\\nlayer transformer encoder (Vaswani et al. 2017). Each fea-\\nture vector at a particular time step is treated as a token, and\\nwe add a fixedD-dimensional sinusoidal positional encod-\\ning pt to mark the chronic order in the lookback window.\\nYu = ||tâˆˆ[1,Ï„]LN(f(Ëœxu,t) +pt),\\nwhere || denotes the concatenation of vectors and LN the\\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\\n164'),\n",
       " Document(id='970cbcc9-8065-5f9e-a43d-67d4ea00eb76', metadata={'author': 'Tong Li, Zhaoyang Liu, Yanyan Shen, Xue Wang, Haokun Chen, Sen Huang', 'creationdate': '2024-03-17T00:07:59+00:00', 'creator': 'PyPDF', 'keywords': 'Application Domains (APP): APP: Other Applications, Data Mining & Knowledge Management (DMKM): DMKM: Mining of Spatial & Temporal or Spatio-Temporal Data', 'moddate': '2024-03-17T00:08:03+00:00', 'page': 7, 'page_label': '8', 'producer': 'PyPDF', 'source': './data/Paper.pdf', 'subject': 'The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)', 'title': 'MASTER: Market-Guided Stock Transformer for Stock Price Forecasting', 'total_pages': 9}, page_content='Acknowledgements\\nThe authors would like to thank the anonymous review-\\ners for their insightful reviews. This work is supported\\nby the National Key Research and Development Program\\nof China (2022YFE0200500), Shanghai Municipal Sci-\\nence and Technology Major Project (2021SHZDZX0102),\\nand SJTU Global Strategic Partnership Fund (2021SJTU-\\nHKUST).\\nReferences\\nBai, S.; Kolter, J. Z.; and Koltun, V . 2018. An empirical\\nevaluation of generic convolutional and recurrent networks\\nfor sequence modeling.arXiv preprint arXiv:1803.01271.\\nBennett, S.; Cucuringu, M.; and Reinert, G. 2022. Leadâ€“\\nlag detection and network clustering for multivariate time\\nseries with an application to the US equity market.Machine\\nLearning, 111(12): 4497â€“4538.\\nBulat, A.; Perez Rua, J. M.; Sudhakaran, S.; Martinez, B.;\\nand Tzimiropoulos, G. 2021. Space-time mixing attention\\nfor video transformer. Advances in neural information pro-\\ncessing systems, 34: 19594â€“19607.\\nChen, T.; and Guestrin, C. 2016. Xgboost: A scalable tree\\nboosting system. In Proceedings of the 22nd acm sigkdd\\ninternational conference on knowledge discovery and data\\nmining, 785â€“794.\\nCho, K.; Van Merri Â¨enboer, B.; Gulcehre, C.; Bahdanau,\\nD.; Bougares, F.; Schwenk, H.; and Bengio, Y . 2014.\\nLearning phrase representations using RNN encoder-\\ndecoder for statistical machine translation. arXiv preprint\\narXiv:1406.1078.\\nCong, Y .; Liao, W.; Ackermann, H.; Rosenhahn, B.; and\\nYang, M. Y . 2021. Spatial-temporal transformer for dynamic'),\n",
       " Document(id='a968b87c-a8f8-54de-a4d6-23d9964d240d', metadata={'author': 'Tong Li, Zhaoyang Liu, Yanyan Shen, Xue Wang, Haokun Chen, Sen Huang', 'creationdate': '2024-03-17T00:07:59+00:00', 'creator': 'PyPDF', 'keywords': 'Application Domains (APP): APP: Other Applications, Data Mining & Knowledge Management (DMKM): DMKM: Mining of Spatial & Temporal or Spatio-Temporal Data', 'moddate': '2024-03-17T00:08:03+00:00', 'page': 1, 'page_label': '2', 'producer': 'PyPDF', 'source': './data/Paper.pdf', 'subject': 'The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)', 'title': 'MASTER: Market-Guided Stock Transformer for Stock Price Forecasting', 'total_pages': 9}, page_content='The return ratio normalizes the market price variety be-\\ntween different stocks in comparison to the absolute price\\nchange. Since stock investment is to rank and select the\\nmost profitable stocks, we perform daily Z-score normaliza-\\ntion of return ratio to encode the label with the rankings,\\nru = NormS(Ëœru), as in previous work (Yang et al. 2020).\\nDefinition 1 (Stock Price Forecasting) Given stock fea-\\ntures {xu,t}uâˆˆS,tâˆˆ[1,Ï„], the stock price forecasting is to\\njointly predict the future normalized return ratio{ru}uâˆˆS.\\nOverview\\nFigure 2 depicts the architecture of our proposed method\\nMASTER, which consists of five steps. (1)Market-Guided\\nGating. We construct a vector representing the current mar-\\nket status mÏ„ and leverage it to rescale feature vectors by\\na gating mechanism, achieving market-guided feature selec-\\ntion. (2)Intra-Stock Aggregation. Within the sequence of\\neach stock, at each time step, we aggregate information from\\nother time steps to generate a local embedding that preserves\\nthe temporal local details of the stock while collecting all\\nimportant signals along the time axis. The local embedding\\nhu,t will serve as relays and transport the collected signals to\\nother stocks in subsequent modules. (3)Inter-Stock Aggre-\\ngation. At each time step, we compute stock correlation with\\nthe attention mechanism, and each stock further aggregates\\nthe local embeddings of other stocks. The aggregated infor-\\nmation zu,t, which we refer to as temporal embedding, con-')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1360: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "res = generate_response(context, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'keypoint': 'MASTER is specialized for stock price forecasting, which is distinct in data form and task properties from existing transformer-based models.',\n",
       "  'sources': 'MASTER is specialized for stock price forecasting, which is distinct in data form and task properties from existing transformer-based models in spatial-temporal data (Bulat et al. 2021; Cong et al. 2021; Xu et al. 2020; Li et al. 2023) or multivariate time series domains (Zhang and Yan 2022; Nie et al. 2022).',\n",
       "  'reasoning': 'This information highlights the unique focus and specialization of the MASTER model compared to existing models.'},\n",
       " {'keypoint': 'The overall computation complexity is O(F MÏ„+ N1MÏ„ 2D2 + N2M2Ï„D2 + MÏ„D 2).',\n",
       "  'sources': 'The overall computation complexity is O(F MÏ„+ N1MÏ„ 2D2 + N2M2Ï„D2 + MÏ„D 2).',\n",
       "  'reasoning': 'This reveals the computational requirements and efficiency analysis of the MASTER model in its operations.'},\n",
       " {'keypoint': 'When compared to the existing stock correlation method (i.e., DTML), MASTER outperforms in all 6 metrics.',\n",
       "  'sources': 'When compared to the existing stock correlation method (i.e., DTML), MASTER outperforms in all 6 metrics, which tells the proposed Market-Guided Gating and aggregation techniques are more efficient in mining cross-stock information than existing literature.',\n",
       "  'reasoning': 'This indicates the effectiveness of the MASTER model by demonstrating its superiority over the DTML method across multiple evaluation metrics.'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['cards'].values[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
